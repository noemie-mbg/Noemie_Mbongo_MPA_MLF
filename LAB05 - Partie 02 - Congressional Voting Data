import pandas as pd
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import tensorflow as tf
import matplotlib.pyplot as plt

# 1. Loading dataset

path_to_dataset = 'drive/MyDrive/MPA-MLF/EX_5/voting_complete.csv' # change the PATH
pd_dataset = pd.read_csv(path_to_dataset)

pd_dataset

# 2. Train/Test Split

def train_test_split(pd_data: pd.DataFrame, test_ratio: float = 0.2) -> tuple: # define a function for train and test split
    pd_dataset = pd_data.copy()
    pd_dataset = pd_dataset[pd_dataset.columns[1:]]
    index = np.arange(len(pd_dataset))
    index = np.random.permutation(index)
    train_ammount = int(len(index)*test_ratio)
    train_ids = index[train_ammount:]
    test_ids = index[:train_ammount]
    
    train_dataset = pd_dataset[pd_dataset.index.isin(train_ids)].reset_index()
    test_dataset = pd_dataset[pd_dataset.index.isin(test_ids)].reset_index()
    
    train_dataset = train_dataset[train_dataset.columns[1:]]
    test_dataset = test_dataset[test_dataset.columns[1:]]

    return train_dataset[train_dataset.columns[1:]], train_dataset[train_dataset.columns[0]], test_dataset[test_dataset.columns[1:]], test_dataset[test_dataset.columns[0]]

x_train, y_train, x_test, y_test = train_test_split(pd_dataset)
print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)

# 3. Data examination

x_train
print("number of rows and columns :", pd_dataset.shape)
print("Data types:", pd_dataset.dtypes)
print("Missing values :", pd_dataset.isnull().sum().sum())  
print("Labels :", pd_dataset['Class Name'].unique())

# 4. Data preprocessing

from sklearn.preprocessing import LabelEncoder

# we replace '?' with the most common value in each column
for col in x_train.columns:
    most_frequent = x_train[col].mode()[0]
    x_train[col] = x_train[col].replace('?', most_frequent)
    x_test[col] = x_test[col].replace('?', most_frequent)


# We encode 'y' in 1 and 'n' in 0
x_train.replace({'y': 1, 'n': 0}, inplace=True)
x_test.replace({'y': 1, 'n': 0}, inplace=True)

# We encode labels
label_encoder = LabelEncoder()
y_train = label_encoder.fit_transform(y_train)
y_test = label_encoder.transform(y_test)

# Display
print(x_train)
print(y_train)

# 5. Creating the model

model = Sequential([
    Dense(16, activation='relu', input_shape=(x_train.shape[1],)),  # hidden layer
    Dense(8, activation='relu'),  # Seconde hidden
    Dense(1, activation='sigmoid')  # classification binary
])

model.summary()

optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)
loss='binary_crossentropy'
metrics=['accuracy']

model.compile(optimizer,loss ,metrics)

history = model.fit(x_train, y_train, validation_split=0.2, epochs=50, batch_size=8)

# 7. Model Evaluation

for col in x_train.columns:
    most_frequent = x_train[col].mode()[0]  
    x_test[col] = x_test[col].replace('?', most_frequent)


from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)  
x_test_scaled = scaler.transform(x_test)  

test_loss, test_accuracy = model.evaluate(x_test_scaled, y_test, verbose=1)

print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

history_dict = history.history

plt.figure(figsize=(10, 5))
plt.plot(history_dict['loss'], label='Training Loss')
plt.plot(history_dict['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training & Validation Loss')
plt.legend()
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(history_dict['accuracy'], label='Training Accuracy')
plt.plot(history_dict['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training & Validation Accuracy')
plt.legend()
plt.show()


